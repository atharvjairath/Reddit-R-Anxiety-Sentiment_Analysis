{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Atharv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Atharv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Atharv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "nltk.download(['punkt','stopwords','wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orignal Journal\n",
    "doc1 = open(\"doc1.txt\",\"r\")\n",
    "doc2 = open(\"doc2.txt\",\"r\")\n",
    "\n",
    "# # For two similar aritcles \n",
    "# doc1 = open(\"doc3.txt\",\"r\")\n",
    "# doc2 = open(\"doc4.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article(file):\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence)\n",
    "    print()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeling very blue today\n",
      "We found out that Barney is very sick\n",
      "Heâ€™s my beloved Golden Retriever pup\n",
      "Well, heâ€™s not a pup anymore, he is 12, but for me he always will be\n",
      "The vet said heâ€™s developed some kinda kidney disease\n",
      "I worry if itâ€™s too late to fix it at this point, the vet seemed worried as well\n",
      "I have been trying to spend as much time with Barney as possible\n",
      "Poor thing seems to be suffering\n",
      "I feel super helpless and useless\n",
      "I just wish we caught it earlier\n",
      "There is a slight glimpse of hope though\n",
      "Another vet said he might still recover\n",
      "Unlikely but it might happen\n",
      "Having a hard time focusing on my school or anything else\n",
      "I tend to lose focus fast even when things are normal but especially right now\n",
      "Everyone is stressing out about Barney and expecting the worst to happen\n",
      "Itâ€™s heartbreaking\n",
      "My younger brother Peter is very emotional\n",
      "I mean Iâ€™m emotional but heâ€™s taking it to another level\n",
      "My parents are sad too\n",
      "He has been part of our lives for so long that itâ€™s hard to imagine otherwise\n",
      "The one thatâ€™s always there for you and loves you unconditionally\n",
      "I seem to be the only one who believes heâ€™s gonna make it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc1_sentences = read_article(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had a huge fight with my mom today\n",
      "I donâ€™t like fighting and tend to avoid conflict\n",
      "But my mom has no problem lashing out\n",
      "She was saying very mean things about my dad\n",
      "My mom & dad are divorced, and he recently started another family\n",
      "I am also struggling to accept it\n",
      "My dad canceled on me AGAIN\n",
      "I was very looking forward to our dinner\n",
      "I should also mention that he JUST had another daughter\n",
      "I understand heâ€™s busy and all, but feeling rather neglected\n",
      "He promised I would always be his priority, but it doesnâ€™t look like it\n",
      "Perhaps I shouldnâ€™t have fought with my mom yesterday\n",
      "Is she right about him? Maybe he is selfish\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc2_sentences = read_article(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    clean_tokens_list = []\n",
    "    for tok in tokens:\n",
    "        lemmatizer_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tok = stemmer.stem(lemmatizer_tok)\n",
    "        clean_tokens_list.append(clean_tok)\n",
    "\n",
    "    return clean_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in tokenize(text)] \n",
    "            for text in doc1_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['feel', 'blue', 'today'],\n",
       " ['found', 'barney', 'sick'],\n",
       " ['belov', 'golden', 'retriev', 'pup'],\n",
       " ['well', 'pup', 'anymor', '12', 'alway'],\n",
       " ['vet', 'said', 'develop', 'kinda', 'kidney', 'diseas'],\n",
       " ['worri', 'late', 'fix', 'point', 'vet', 'seem', 'worri', 'well'],\n",
       " ['tri', 'spend', 'much', 'time', 'barney', 'possibl'],\n",
       " ['poor', 'thing', 'seem', 'suffer'],\n",
       " ['feel', 'super', 'helpless', 'useless'],\n",
       " ['wish', 'caught', 'earlier'],\n",
       " ['slight', 'glimps', 'hope', 'though'],\n",
       " ['anoth', 'vet', 'said', 'might', 'still', 'recov'],\n",
       " ['unlik', 'might', 'happen'],\n",
       " ['hard', 'time', 'focus', 'school', 'anyth', 'els'],\n",
       " ['tend',\n",
       "  'lose',\n",
       "  'focu',\n",
       "  'fast',\n",
       "  'even',\n",
       "  'thing',\n",
       "  'normal',\n",
       "  'especi',\n",
       "  'right'],\n",
       " ['everyon', 'stress', 'barney', 'expect', 'worst', 'happen'],\n",
       " ['heartbreak'],\n",
       " ['younger', 'brother', 'peter', 'emot'],\n",
       " ['mean', 'emot', 'take', 'anoth', 'level'],\n",
       " ['parent', 'sad'],\n",
       " ['part', 'life', 'long', 'hard', 'imagin', 'otherwis'],\n",
       " ['one', 'alway', 'love', 'uncondit'],\n",
       " ['seem', 'one', 'belief', 'gon', 'na', 'make']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 0, 'feel': 1, 'today': 2, 'barney': 3, 'found': 4, 'sick': 5, 'belov': 6, 'golden': 7, 'pup': 8, 'retriev': 9, '12': 10, 'alway': 11, 'anymor': 12, 'well': 13, 'develop': 14, 'diseas': 15, 'kidney': 16, 'kinda': 17, 'said': 18, 'vet': 19, 'fix': 20, 'late': 21, 'point': 22, 'seem': 23, 'worri': 24, 'much': 25, 'possibl': 26, 'spend': 27, 'time': 28, 'tri': 29, 'poor': 30, 'suffer': 31, 'thing': 32, 'helpless': 33, 'super': 34, 'useless': 35, 'caught': 36, 'earlier': 37, 'wish': 38, 'glimps': 39, 'hope': 40, 'slight': 41, 'though': 42, 'anoth': 43, 'might': 44, 'recov': 45, 'still': 46, 'happen': 47, 'unlik': 48, 'anyth': 49, 'els': 50, 'focus': 51, 'hard': 52, 'school': 53, 'especi': 54, 'even': 55, 'fast': 56, 'focu': 57, 'lose': 58, 'normal': 59, 'right': 60, 'tend': 61, 'everyon': 62, 'expect': 63, 'stress': 64, 'worst': 65, 'heartbreak': 66, 'brother': 67, 'emot': 68, 'peter': 69, 'younger': 70, 'level': 71, 'mean': 72, 'take': 73, 'parent': 74, 'sad': 75, 'imagin': 76, 'life': 77, 'long': 78, 'otherwis': 79, 'part': 80, 'love': 81, 'one': 82, 'uncondit': 83, 'belief': 84, 'gon': 85, 'make': 86, 'na': 87}\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (5, 1)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(8, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(13, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2)],\n",
       " [(3, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)],\n",
       " [(23, 1), (30, 1), (31, 1), (32, 1)],\n",
       " [(1, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(36, 1), (37, 1), (38, 1)],\n",
       " [(39, 1), (40, 1), (41, 1), (42, 1)],\n",
       " [(18, 1), (19, 1), (43, 1), (44, 1), (45, 1), (46, 1)],\n",
       " [(44, 1), (47, 1), (48, 1)],\n",
       " [(28, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1)],\n",
       " [(32, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1)],\n",
       " [(3, 1), (47, 1), (62, 1), (63, 1), (64, 1), (65, 1)],\n",
       " [(66, 1)],\n",
       " [(67, 1), (68, 1), (69, 1), (70, 1)],\n",
       " [(43, 1), (68, 1), (71, 1), (72, 1), (73, 1)],\n",
       " [(74, 1), (75, 1)],\n",
       " [(52, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1)],\n",
       " [(11, 1), (81, 1), (82, 1), (83, 1)],\n",
       " [(23, 1), (82, 1), (84, 1), (85, 1), (86, 1), (87, 1)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique id and their frequency\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['blue', 0.62], ['feel', 0.48], ['today', 0.62]]\n",
      "[['barney', 0.42], ['found', 0.64], ['sick', 0.64]]\n",
      "[['belov', 0.53], ['golden', 0.53], ['pup', 0.41], ['retriev', 0.53]]\n",
      "[['pup', 0.4], ['12', 0.51], ['alway', 0.4], ['anymor', 0.51], ['well', 0.4]]\n",
      "[['develop', 0.45], ['diseas', 0.45], ['kidney', 0.45], ['kinda', 0.45], ['said', 0.35], ['vet', 0.29]]\n",
      "[['well', 0.27], ['vet', 0.22], ['fix', 0.34], ['late', 0.34], ['point', 0.34], ['seem', 0.22], ['worri', 0.69]]\n",
      "[['barney', 0.29], ['much', 0.45], ['possibl', 0.45], ['spend', 0.45], ['time', 0.35], ['tri', 0.45]]\n",
      "[['seem', 0.37], ['poor', 0.57], ['suffer', 0.57], ['thing', 0.45]]\n",
      "[['feel', 0.41], ['helpless', 0.53], ['super', 0.53], ['useless', 0.53]]\n",
      "[['caught', 0.58], ['earlier', 0.58], ['wish', 0.58]]\n",
      "[['glimps', 0.5], ['hope', 0.5], ['slight', 0.5], ['though', 0.5]]\n",
      "[['said', 0.38], ['vet', 0.32], ['anoth', 0.38], ['might', 0.38], ['recov', 0.49], ['still', 0.49]]\n",
      "[['might', 0.52], ['happen', 0.52], ['unlik', 0.67]]\n",
      "[['time', 0.34], ['anyth', 0.44], ['els', 0.44], ['focus', 0.44], ['hard', 0.34], ['school', 0.44]]\n",
      "[['thing', 0.27], ['especi', 0.34], ['even', 0.34], ['fast', 0.34], ['focu', 0.34], ['lose', 0.34], ['normal', 0.34], ['right', 0.34], ['tend', 0.34]]\n",
      "[['barney', 0.29], ['happen', 0.35], ['everyon', 0.45], ['expect', 0.45], ['stress', 0.45], ['worst', 0.45]]\n",
      "[['heartbreak', 1.0]]\n",
      "[['brother', 0.53], ['emot', 0.41], ['peter', 0.53], ['younger', 0.53]]\n",
      "[['anoth', 0.38], ['emot', 0.38], ['level', 0.49], ['mean', 0.49], ['take', 0.49]]\n",
      "[['parent', 0.71], ['sad', 0.71]]\n",
      "[['hard', 0.33], ['imagin', 0.42], ['life', 0.42], ['long', 0.42], ['otherwis', 0.42], ['part', 0.42]]\n",
      "[['alway', 0.43], ['love', 0.56], ['one', 0.43], ['uncondit', 0.56]]\n",
      "[['seem', 0.29], ['one', 0.35], ['belief', 0.45], ['gon', 0.45], ['make', 0.45], ['na', 0.45]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity(r'C:\\Users\\Atharv\\Desktop\\Reddit-',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Had a huge fight with my mom today',\n",
       " 'I donâ€™t like fighting and tend to avoid conflict',\n",
       " 'But my mom has no problem lashing out',\n",
       " 'She was saying very mean things about my dad',\n",
       " 'My mom & dad are divorced, and he recently started another family',\n",
       " 'I am also struggling to accept it',\n",
       " 'My dad canceled on me AGAIN',\n",
       " 'I was very looking forward to our dinner',\n",
       " 'I should also mention that he JUST had another daughter',\n",
       " 'I understand heâ€™s busy and all, but feeling rather neglected',\n",
       " 'He promised I would always be his priority, but it doesnâ€™t look like it',\n",
       " 'Perhaps I shouldnâ€™t have fought with my mom yesterday',\n",
       " 'Is she right about him? Maybe he is selfish',\n",
       " '\\n']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sims = []\n",
    "# for line in doc2_sentences:\n",
    "#     query_doc = [w.lower() for w in tokenize(line)] \n",
    "#     query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "#     query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "#     print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "#     sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "#     avg = sum_of_sims / len(doc1_sentences)\n",
    "#     print(f'avg: {sum_of_sims / len(doc1_sentences)}')\n",
    "#     avg_sims.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "# percentage_of_similarity = round(float(total_avg) * 100)\n",
    "# if percentage_of_similarity >= 100:\n",
    "#     percentage_of_similarity = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_of_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = [[w.lower() for w in tokenize(text)] \n",
    "            for text in doc2_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['huge', 'fight', 'mom', 'today'],\n",
       " ['like', 'fight', 'tend', 'avoid', 'conflict'],\n",
       " ['mom', 'problem', 'lash'],\n",
       " ['say', 'mean', 'thing', 'dad'],\n",
       " ['mom', 'dad', 'divorc', 'recent', 'start', 'anoth', 'famili'],\n",
       " ['also', 'struggl', 'accept'],\n",
       " ['dad', 'cancel'],\n",
       " ['look', 'forward', 'dinner'],\n",
       " ['also', 'mention', 'anoth', 'daughter'],\n",
       " ['understand', 'busi', 'feel', 'rather', 'neglect'],\n",
       " ['promis', 'would', 'alway', 'prioriti', 'look', 'like'],\n",
       " ['perhap', 'fought', 'mom', 'yesterday'],\n",
       " ['right', 'mayb', 'selfish'],\n",
       " []]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_bow = [dictionary.doc2bow(query_doc) for query_doc in query_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x16ea8511248>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [[0.61937135 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34086362 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27504158 0.         0.         0.         0.\n",
      "  0.         0.         0.16315883 0.         0.         0.\n",
      "  0.38433212 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.3781851\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.379473   0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.3781851\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.379473   0.         0.         0.         0.        ]\n",
      " [0.48245022 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.41015115 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.39852646 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4345238  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34086362 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "Average Similarity Score: 38 %\n"
     ]
    }
   ],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "avg = sum_of_sims / len(doc2_sentences)\n",
    "print(\"Average Similarity Score:\",round(float(avg) * 100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
